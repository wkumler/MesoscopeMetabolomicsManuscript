---
title: "Control Script for SCOPE data"
output: html_document
---

```{r setup, include=FALSE, purl=TRUE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = TRUE, purl = FALSE)
options(dplyr.summarise.inform=F)
options(pillar.sigfig=10)
suppressPackageStartupMessages(library(tidyverse))
library(data.table)
library(pbapply)
library(RaMS)

polarity <- "neg"
cruise <- "MC"
output_folder <- paste0(c(cruise, polarity, "output/"), collapse = "_")
mzml_path <- paste0("../mzMLs/", polarity, "/")

source("scripts/functions.R")

cruise_metadata <- read.csv("../metadata/made_data/file_metadata.csv") %>%
  filter(cruise==!!cruise)

run_date <- cruise_metadata %>%
  pull(date_run) %>%
  unique()

ms_files <- cruise_metadata$filename

BiocParallel::register(BPPARAM = BiocParallel::SnowParam(tasks = length(ms_files), progressbar = TRUE))
```

## Peakpicking

**Obviously** the most important step, you can't do anything else without first finding things to look at and talk about. The script below, when sourced, runs XCMS's centWave peakpicking. It also calculates an improved signal-to-noise and metric of Gaussian-ness that seems to sift through the noise more accurately than the default signal-to-noise ratio, which has known bugs (https://doi.org/10.1021/acs.analchem.7b01069). Finally, it performs retention time correction and peak correspondence (grouping peaks across files).

It takes in the metadataframe created above as well as the list of paths to the .mzML files and returns a (rather large) data frame full of peaks. 

```{r peakpicking, error=TRUE, purl=FALSE}
library(xcms)

# Create folder if it doesn't already exist
if(!dir.exists(output_folder)){dir.create(output_folder)}

# Define the peakpicking parameters
cwp <- CentWaveParam(ppm = 2.5, peakwidth = c(15, 45), 
                     snthresh = 1, prefilter = c(5, 10000), 
                     integrate = 2, mzCenterFun = "wMean", 
                     mzdiff = 0.001, fitgauss = FALSE, 
                     noise = 5000, firstBaselineCheck = FALSE, 
                     extendLengthMSW = TRUE)

# Set the new quality threshold
qscore_threshold <- 30

# Define the retention time correction parameters
obp <- ObiwarpParam(binSize = 0.1, centerSample = length(ms_files)/2, 
                    response = 1, distFun = "cor_opt")

# Define the correspondence parameters
pdp <- PeakDensityParam(sampleGroups = cruise_metadata$samp_type, 
                        bw = 12, minFraction = 0.1, 
                        binSize = 0.001, minSamples = 2)

# Make sure that filled peaks have at least a 2.5ppm window from mzmin to mzmax around mz
fpp <- FillChromPeaksParam(ppm = 2.5)


# Perform peakpicking
source("scripts/peakpicking.R")
unique(warnings())

# Fill in missing data - filename/feature combinations with no entry
# First, choose largest peak from those with multiples per file
# Second, remove all peaks that show up in less than 50% of the *samples*
# Fill in mz with the average for each feature
# Fill in retention times using samples with nearest timestamp
n_samps <- length(str_subset(unique(cruise_metadata$filename), "Smp"))
good_feats <- raw_peaks %>%
  left_join(cruise_metadata) %>%
  filter(samp_type=="Smp") %>%
  count(feature) %>%
  filter(n>floor(n_samps*0.5)) %>%
  distinct(feature)
filled_peaks <- good_feats %>%
  left_join(raw_peaks) %>%
  arrange(desc(into)) %>%
  group_by(feature, filename) %>%
  slice(1) %>%
  ungroup() %>%
  complete(feature, filename) %>%
  left_join(cruise_metadata) %>%
  arrange(timestamp) %>%
  group_by(feature) %>%
  mutate(mz=ifelse(is.na(mz), mean(mz, na.rm=TRUE), mz)) %>%
  fill(starts_with("rt"), .direction = "downup") %>%
  arrange(feature) %>%
  select(feature, filename, mz, rt, rtmin, rtmax)

# Save intermediate results
saveRDS(xdata, file = paste0(output_folder, "xdata.rds"))
saveRDS(files_qscores, file = paste0(output_folder, "files_qscores.rds"))
saveRDS(xdata_cleanpeak, file = paste0(output_folder, "xdata_cleanpeak.rds"))
saveRDS(xdata_rt, file = paste0(output_folder, "xdata_rt.rds"))
saveRDS(xdata_cor, file = paste0(output_folder, "xdata_cor.rds"))
saveRDS(xdata_filled, file = paste0(output_folder, "xdata_filled.rds"))
write.csv(raw_peaks, file = paste0(output_folder, "raw_peaks.csv"), row.names = FALSE)
write.csv(filled_peaks, file = paste0(output_folder, "filled_peaks.csv"), row.names = FALSE)
unique(warnings())
```

## De-isotoping and de-adducting

After peaks have been identified, many of them will be isotopes and adducts of other peaks. Removing these is important to minimize pseudoreplication issues and data artifacts. I didn't like any of the packages I tried to do this, so I wrote my own code to process this more robustly.

This script does two things. First, it identifies peaks that are likely isotopes or adducts of other peaks in the data set. It does this by assuming that every single peak is an isotope/adduct of another peak. If a peak is an adduct/isotope, there will be another peak that looks very similar but is separated by a very specific mass difference. For example, if we find a peak at 140.06875, we "guess" that it's an adduct or isotope and check all the places where the M+H would be found, depending on the isotope or adduct. If we suspect it of being a 13C isotope of another peak, we'd look for a peak at 140.06875-1.003355. If we suspect it of being a sodium isotope, we'd look for a peak at 140.06875-22.99787+1.007276. In this case, we would indeed find a signal at the M+H peak if we assume sodium, but no peak at the M+H if we assume a 13C isotope - allowing us to conclude tentatively that this is actually an adduct.

However, simply finding data at the expected mass isn't specific enough because we'll often stumble upon noise or a different peak in the general area. Thus, we check for peak similarity before assuming that there's real signal there. The general theory is that isotopes and adducts will be similar to the base peak in two ways. First, the individual x/y (rt/intensity) data points should match up almost exactly, as adducts and isotopes elute at the same time as the base peak. We can check this with a Pearson correlation - simply, how nicely do the two peaks correlate with each other? This method is quite sensitive to even small differences in retention time. Second, we can compare peak area ratios across files. Here, we run a similar correlation but check x/y as base peak area/isotope area. This works because a compound will always create similarly stable adducts (some will love M+H, others M+Na) and will always have a fixed isotope abundance. Across multiple files, the correlation between these should be very strong (and is, in fact, usually stronger than the first method).

Deciding on the thresholds to use as cutoffs for "yes, this is an adduct/isotope" is complicated. If set too high, not all adducts and isotopes will be flagged and removed - spuriously increasing the impact of a compound that has an addiso that snuck through. If set too low, compounds that aren't adducts or isotopes will be erroneously removed. Ideally, there's a clear separation between the spurious peaks and the ones that are good - spurious all hovering around zero and good all close to 1. However, it seems like there's a lot more overlap and so there's two things we can do. First, plotting histograms of peak area and shape correlations can provide some clue as to where the valley in between the two is and that's a good starting spot. For a more intensive analysis we can use our standards to calibrate the thresholds because we can predict the isotope envelope of each standard and know what isotopes to expect and which ones shouldn't show up at all. 

```{r deisoadduct, error=TRUE, purl=FALSE}
library(xcms)
xdata_filled <- readRDS(file = paste0(output_folder, "xdata_filled.rds"))
filled_peaks <- read.csv(file = paste0(output_folder, "filled_peaks.csv"))

# How similar in shape do the original and addiso peaks need to be?
shape_remove_threshold <- 0.8
# How good does the M area ~ adduct area correlation across files need to be to assume adduct?
area_remove_threshold <- 0.95

# Which adducts and isotopes should we look for?
addiso_masses_all <- tribble(
  ~adduct_or_iso, ~polarity, ~addiso, ~nat_abund, ~mz_diff,
  "iso",          "both",    "C13",   0.0110,     1.003355, 
  "iso",          "both",    "X2C13", 0.0110,     2*1.003355, 
  "iso",          "both",    "S34",   0.0421,     1.995796, 
  "iso",          "both",    "S33",   0.0075,     0.999387,
  "iso",          "both",    "N15",   0.0037,     0.997035, 
  "iso",          "both",    "O18",   0.0020,     2.004244,
  "iso",          "both",    "Cl37",  0.2423,     1.99705,
  "iso",          "both",    "Br81",  0.4931,     1.997954,
  "adduct",       "both",    "M-H2O", NA,         -18.0106, 
  "adduct",       "pos",     "M+Na",  NA,          22.98922-1.007276 # Goes from [M+H] to [M+Na]
  # "adduct",       "pos",     "M+NH4", NA,          18.0338-1.007276,
  # "adduct",       "pos",     "M+K",   NA,          38.963708-1.007276,
  # "adduct",       "neg",     "M+Cl",  NA,          34.969402+1.007276,
  # "adduct",       "neg",     "M+Ac",  NA,          59.013851+1.007276,
  # "adduct",       "neg",     "M+Br",  NA,          78.918885+1.007276
)
addiso_masses <- addiso_masses_all %>% filter(polarity%in%c(!!polarity, "both"))

# Make sure certain known compounds aren't removed spuriously
not_addisos <- tribble(
  ~name,           ~mz,        ~addiso,
  "Glutamine",     147.076968, "M+NH4",
  "Citrulline",    176.103517, "M+NH4",
  "Guanine",       152.0567,   "M+NH4",
  "Glutamic acid", 148.061,    "M+NH4",
  "4-Aminobutyric acid", 104.071, "M+NH4",
  "Adenine",       136.0618,   "M+NH4",
) %>%
  pmap_dfr(function(...){
    row_data <- data.frame(...)
    filled_peaks %>%
      filter(mz%between%pmppm(row_data$mz, 5)) %>%
      mutate(name=row_data$name, addiso=row_data$addiso)
  }) %>% distinct(feature, name, addiso)

# Run the script
source("scripts/deisoadduct.R")
unique(warnings())

# Some summary stats
nrow(has_addiso)
nrow(is_addiso)
sort(table(has_addiso$addiso), decreasing = TRUE)

# Save intermediate files
saveRDS(isoadduct_df, file = paste0(output_folder, "isoadduct_df.rds"))

# Write out addiso information
write.csv(is_addiso, file = paste0(output_folder, "is_addiso.csv"), row.names = FALSE)
write.csv(has_addiso, file = paste0(output_folder, "has_addiso.csv"), row.names = FALSE)
write.csv(M_areas, file = paste0(output_folder, "M_areas.csv"), row.names = FALSE)
# Contains feature x sample x addiso data frame
write.csv(feature_envelopes, file = paste0(output_folder, "feature_envelopes.csv"), row.names = FALSE)
# Update filled_peaks with reintegrated areas
M_area_peaks <- filled_peaks %>% left_join(M_areas)
write.csv(M_area_peaks, paste0(output_folder, "M_area_peaks.csv"), row.names = FALSE)

unique(warnings())
```

```{r addiso check}
given_file <- str_subset(unique(filled_peaks$filename), "Poo_Full")
file_data_table <- as.data.table(filterFile(xdata_filled, given_file))[,c("rt", "mz", "i")]
file_peaks <- filled_peaks %>% filter(filename%in%given_file)

# test_peak <- file_peaks %>% filter(feature=="FT0207")
test_peak <- file_peaks %>% filter(feature=="FT0702")
feature_i <- test_peak$feature
mz_i <- test_peak$mz
rtmin_i <- test_peak$rtmin
rtmax_i <- test_peak$rtmax
split_table_i <- file_data_table

addiso_masses %>%
  {rbind(cbind(., type="self", dir=-1), cbind(., type="seek", dir=1))} %>%
  mutate(mz_diff=mz_diff*dir) %>%
  select(-dir) %>%
  add_row(polarity="both", addiso="M+/-H", mz_diff=0, type="self") %>%
  group_by(addiso, type) %>%
  group_split() %>% 
  lapply(function(row_data){
    v <- split_table_i[
      mz%between%pmppm(row_data$mz_diff+mean(mz_i), 5)][
        rt%between%c(mean(rtmin_i), mean(rtmax_i))]
    v$addiso <- row_data$addiso
    v$type <- row_data$type
    v
  }) %>%
  rbindlist() %>%
  filter(!is.na(i)) %>%
  group_by(addiso, type) %>%
  filter(n()>=5) %>%
  ggplot(aes(x=rt, y=i)) +
  geom_path() +
  geom_point() +
  facet_grid(addiso~type, scales = "free_y")
```

## Noise audit peakpicking?

```{r peakpicking noise audit, eval=FALSE}
filled_peaks <- read.csv(paste0(output_folder, "filled_peaks.csv"))

filled_peaks %>%
  group_by(feature) %>%
  summarise(mean_mz=mean(mz), mean_rt=mean(rt)/60, med_sn=median(sn,na.rm = TRUE)) %>%
  filter(!mean_rt%between%c(14.5, 14.7)) %>%
  filter(mean_rt>1) %>%
  arrange(desc(med_sn))

mini_mzml_names <- paste(output_folder, ms_files)
set.seed(123)
feats_to_check <- slice_sample(picked_feats, n = 100)
```

```{r check peakpicking}
filled_peaks <- read.csv(paste0(output_folder, "filled_peaks.csv"))

picked_feats <- filled_peaks %>%
  filter(str_detect(filename, "Smp")) %>%
  group_by(feature) %>%
  summarise(mean_mz=mean(mz), mean_rt=mean(rt)/60)

picked_feats %>%
  ggplot() + 
  geom_point(aes(x=mean_rt, y=mean_mz)) +
  xlim(0, NA)

# msdata <- grabMSdata(list.files("../tmzMLs/pos", pattern = "180821", full.names = TRUE))
# test_data <- slice_sample(picked_feats, n = 1)
# test_data <- picked_feats %>% filter(feature=="FT0179")
# bet_data <- msdata$MS1[mz%between%pmppm(test_data$mean_mz, 5)]
# bet_data %>%
#   filter(rt%between%c(8, 13)) %>%
#   mutate(filename=str_replace(filename, "tmzML", "mzML")) %>%
#   left_join(cruise_metadata) %>%
#   ggplot() +
#   geom_line(aes(x=rt, y=int, group=filename, color=samp_type)) +
#   geom_vline(xintercept = test_data$mean_rt) +
#   theme_bw() +
#   ggtitle(do.call(what = paste, test_data))
```

## Annotate standards

This block runs a custom function that tries to resolve discrepancies between various metrics using basically a bunch of if/else statements.
  - Highest priority: isotope validated (isotope_choice)
  - Next highest: if two peaks are found and two standards should be found, use RT ordering to match them up (match_choice)
  - Next highest: if peaks are dramatically larger in the correct mix and smaller in the other mix, assume it's the larger one (mix_choice) sometimes produces multiple, thus later area and RT matching
  - Next highest: if one peak is larger than the others, assume it's the
              standard (area_choice)
  - Lowest: RT matching based on the number in the standards list

For each standard, grab all the features that it could be based on 5ppm m/z window. If there aren't any features, return NA for everything. If there are features, calculate 5 metrics for each one:
Isotope choice:
  If the compound has an isotope, use the peak closest to the isotope peak in RT
    (i.e. another compound exists with the same name + C13, N15, or O18)
  If the compound IS an isotope (i.e. has C13, N15, or O18 in the name)
    find the two peaks closest in RT and assume those are the isotopologues
  Otherwise, NA
Match choice:
  If there are two standards expected in a mix and two features found,
  assume that the two found peaks are the standards and use rank-order RT matching
  If more or fewer peaks found, NA
Mix choice:
  Generally, which peaks are bigger in the mix they've been added to?
  Calculate average z-statistic between the mixes after controlling for H2O vs Matrix
  Return all feature numbers with a z-score above 10 (arbitrarily chosen)
Area choice:
  Which peak has the largest area?
RT choice:
  Which peak is closest in RT to the value in the standards list?


```{r targeted, purl=TRUE}
M_area_peaks <- read.csv(paste0(output_folder, "M_area_peaks.csv"))
given_stans <- read.csv("../metadata/made_data/clean_stans.csv") %>% 
  filter(polarity==!!polarity) %>%
  filter(run_date[1]>=date_added)

feature_data <- M_area_peaks %>%
  group_by(feature) %>%
  summarise(mzmed=median(mz), rtmed=round(median(rt)), avgarea=round(mean(M_area)))

source("scripts/standard_assignments.R")

write.csv(stan_annotations, paste0(output_folder, "stan_annotations.csv"), 
          row.names = FALSE)
```

```{r useful dedup code}
library(xcms)
xdata_filled <- readRDS(file = paste0(output_folder, "xdata_filled.rds"))
pooled_msdata <- str_subset(ms_files, "Std") %>%
  lapply(function(filename){
    filterFile(xdata_filled, filename) %>%
      as.data.frame() %>%
      mutate(filename=filename)
  }) %>%
  rbindlist()

mz_i <- given_stans %>%
  filter(compound_name=="Trehalose") %>%
  pull(mz)
# mz_i <- 341.1084

# Plot chromatogram
peak_bounds <- M_area_peaks %>% 
  filter(mz%between%pmppm(mz_i, 10)) %>%
  filter(str_detect(filename, "Std"))
pooled_msdata[mz%between%pmppm(mz_i, 20)] %>%
  ggplot() + geom_line(aes(x=rt, y =i, group=filename)) +
  # facet_wrap(~filename) +
  facet_wrap(~filename, ncol=2) +
  theme_bw() +
  geom_vline(aes(xintercept=rtmin), color="green", data = peak_bounds) +
  geom_vline(aes(xintercept=rtmax), color="red", data = peak_bounds)
M_area_peaks %>% 
  filter(mz%between%pmppm(mz_i, 10)) %>% 
  group_by(feature) %>%
  summarize(mzmed=median(mz), rtmed=median(rt), 
            areamed=median(M_area, na.rm=TRUE),
            minrt=median(rtmin), maxrt=median(rtmax)) %>%
  arrange(rtmed)

# Plotting peak areas by file type
M_area_peaks %>%
  filter(mz%between%pmppm(mz_i)) %>%
  mutate(fillcol=str_extract(filename, "Mix1|Mix2|Poo|Smp|Blk|H2O|Stds")) %>%
  mutate(fillcol=factor(fillcol, levels=unique(fillcol))) %>%
  mutate(feature=factor(feature, levels=unique(peak_bounds$feature))) %>%
  ggplot() +
  geom_col(aes(x=filename, y=M_area, fill=fillcol)) +
  theme(axis.text.x = element_text(angle=90)) +
  facet_wrap(~feature, ncol=1, scales = "free_y")

# Grab possible stans that should have been annotated
given_stans %>%
  filter(mz%between%pmppm(mz_i, 20)) %>%
  arrange(rt)
```

```{r how many annod}
cat(paste0("Annotated: ", sum(!is.na(stan_annotations$feature)),
           "\nUnannotated: ", sum(is.na(stan_annotations$feature))))
stan_annotations %>% filter(is.na(stan_annotations$feature))
```



## Finding B-MISs

```{r B-MIS, purl=TRUE}
M_area_peaks <- read.csv(paste0(output_folder, "M_area_peaks.csv"))
stan_annotations <- read.csv(paste0(output_folder, "stan_annotations.csv"))
given_stans <- read.csv("../metadata/made_data/clean_stans.csv")
is_peaks <- read.csv(paste0("IS_integrations/all_is.csv")) %>%
  filter(cruise==!!cruise) %>%
  mutate(filename=paste0(Replicate.Name, ".mzML")) %>%
  left_join(given_stans, by=c("Precursor.Ion.Name"="compound_name_old")) %>%
  select(filename, compound_name, M_area=Area) %>%
  mutate(M_area=as.numeric(M_area)) %>%
  mutate(M_area=ifelse(is.na(M_area), 0, M_area)) %>%
  group_by(compound_name) %>%
  mutate(M_area=ifelse(M_area==0, mean(M_area), M_area)) %>%
  ungroup()

min_improvement <- 0.2
already_good <- 0.1 #Not currently used

source("scripts/bmisscript.R")

table(chosen_BMIS$BMIS)
table(chosen_BMIS$BMIS!="None")

write.csv(is_peaks, file = paste0(output_folder, "is_peaks.csv"), 
          row.names = FALSE)
write.csv(chosen_BMIS, file = paste0(output_folder, "chosen_BMIS.csv"), 
          row.names = FALSE)
```

```{r bmischeck}
is_peaks <- read.csv(paste0(output_folder, "is_peaks.csv"))

# Create plot of absolute IS areas
facet_labels <- paste(unique(is_peaks$compound_name), sep=": ")
names(facet_labels) <- unique(is_peaks$compound_name)
IS_areas_gp <- is_peaks %>%
  mutate(type=str_extract(filename, "Blk|Poo|Smp|Std")) %>%
  mutate(xax=str_extract(filename, "(?<=_.{3}_).*(?=\\.mzML)")) %>%
  # mutate(xax=paste0(str_extract(filename, "^\\d+"), "_", xax)) %>%
  mutate(xax=factor(xax, levels = unique(xax))) %>%
  ggplot() +
  geom_bar(aes(x=xax, y=M_area, fill=type), stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust=1, vjust=0.5)) +
  facet_wrap(~compound_name, ncol = 1, scales = "free_y",
             labeller = as_labeller(facet_labels))

IS_areas_gp

ggsave(plot = IS_areas_gp, filename = paste0(output_folder, "IS_areas.pdf"), 
       device = "pdf", width = 10, height = 10, dpi = 144)


# Create BMIS plot for IS with pooled samps vs all samps
IS_pooled_cvs <- is_peaks %>% 
  filter(str_detect(filename, "_Poo_")) %>%
  select(compound_name, M_area, filename) %>%
  {full_join(., ., by=c("filename"))} %>%
  group_by(compound_name.x, compound_name.y) %>%
  mutate(M_area_normed=(M_area.x/M_area.y)*mean(M_area.y)) %>%
  summarise(calc_cv=cv(M_area_normed)) %>%
  mutate(type="pooled_only")
IS_all_cvs <- is_peaks %>% 
  filter(!str_detect(filename, "Blk")) %>%
  select(compound_name, M_area, filename) %>%
  filter(M_area>0) %>%
  {full_join(., ., by=c("filename"))} %>%
  group_by(compound_name.x, compound_name.y) %>%
  mutate(M_area_normed=(M_area.x/M_area.y)*mean(M_area.y)) %>%
  summarise(calc_cv=cv(M_area_normed)) %>%
  mutate(type="all_files")
BMIS_on_IS_gp <- rbind(IS_pooled_cvs, IS_all_cvs) %>%
  pivot_wider(values_from = calc_cv, names_from = "type") %>%
  ggplot() +
  geom_point(aes(x=pooled_only, y=all_files, label=compound_name.y)) +
  facet_wrap(~compound_name.x, scales = "free") +
  geom_hline(yintercept = 1) +
  geom_vline(xintercept = 1) +
  geom_abline(slope = 1, intercept = 0)
BMIS_on_IS_gp
ggsave(plot=BMIS_on_IS_gp, filename = paste0(output_folder, "BMIS_on_IS.pdf"), 
       device = "pdf", width = 8.5, height = 8.5)

# plotly::ggplotly(IS_areas_gp)
# plotly::ggplotly(BMIS_on_IS_gp)
```



## Annotate formulae

```{r assignformulae, purl=FALSE}
M_area_peaks <- read.csv(paste0(output_folder, "M_area_peaks.csv"))
feature_envelopes <- read.csv(paste0(output_folder, "feature_envelopes.csv"))
feature_data <- M_area_peaks %>%
  group_by(feature) %>%
  summarise(mzmed=median(mz), rtmed=median(rt), avg_area=mean(M_area))
stan_annotations <- read.csv(paste0(output_folder, "stan_annotations.csv"))
# database_formulae <- readRDS("unique_formulae.rds") %>%
#   gsub(pattern = "\\+.?", replacement = "")
database_formulae <- unique(readRDS("../metadata/raw_data/hmdb_formulae.rds"))

source("scripts/formula_assignments.R")

saveRDS(sirius_formulas, file = paste0(output_folder, "sirius_formulas.rds"))
saveRDS(rdisop_formulas, file = paste0(output_folder, "rdisop_formulas.rds"))
saveRDS(manual_check, file = paste0(output_folder, "iso_formulas.rds"))
write.csv(feature_formulas, file = paste0(output_folder, "feature_formulas.csv"), row.names = FALSE)
```

```{r assessformulas}
feature_formulas <- read.csv(paste0(output_folder, "feature_formulas.csv"))
stan_annotations <- read.csv(paste0(output_folder, "stan_annotations.csv")) %>%
  filter(!is.na(feature)) %>%
  select(compound_name, feature)
given_stans <- read.csv("../metadata/made_data/clean_stans.csv") %>% 
  filter(polarity==!!polarity) %>%
  filter(compound_type!="Internal Standard") %>%
  select(compound_name, compound_type, official_formula=formula)

anno_quality <- stan_annotations %>%
  left_join(feature_formulas, by="feature") %>%
  left_join(given_stans,by="compound_name") %>%
  filter(!is.na(official_formula)) %>%
  select(compound_name, feature, official_formula, formula) %>%
  rowwise() %>%
  mutate(form_diffH=ifelse(polarity=="pos", minH(formula), addH(formula))) %>%
  mutate(success=case_when(
    is.na(formula) ~ "Unannotated",
    formula==official_formula | form_diffH==official_formula ~ "Success",
    TRUE ~ "Failed"
  )) %>%
  print(n=Inf)

anno_quality %>% count(success)
anno_quality %>% filter(success=="Failed")
```

## Annotate classes and structures

```{r assignids}
filled_peaks <- read.csv(paste0(output_folder, "filled_peaks.csv"))
feature_envelopes <- read.csv(paste0(output_folder, "feature_envelopes.csv"))
feature_data <- filled_peaks %>%
  group_by(feature) %>%
  summarise(mzmed=median(mz), rtmed=median(rt), avg_area=mean(M_area)) %>%
  left_join(feature_envelopes, by = "feature")
stan_annotations <- read.csv(paste0(output_folder, "stan_annotations.csv"))


MSMS_files <- paste0(mzml_path, "/MSMS") %>%
  list.files(pattern = unique(str_extract(ms_files, "^\\d+"))[1], 
             full.names = TRUE)
sirius_project_dir <- paste0(output_folder, "/sirius_project")

source("scripts/run_sirius.R")

write.csv(classy_confs, file = paste0(output_folder, "classy_confs.csv"), row.names = FALSE)
write.csv(structure_ids, file = paste0(output_folder, "structure_ids.csv"), row.names = FALSE)
```

```{r assessclasses}
stan_classes <- read.csv(paste0("https://raw.githubusercontent.com/",
                             "IngallsLabUW/Ingalls_Standards/",
                             "b098927ea0089b6e7a31e1758e7c7eaad5408535/",
                             "Ingalls_Lab_Standards_NEW.csv")) %>%
  filter(Column=="HILIC") %>%
  mutate(polarity=tolower(gsub(pattern = "HILIC", "", .$Fraction1))) %>%
  mutate(m.z=as.numeric(m.z)) %>%
  select(compound_name=Compound.Name, classyfire=Classyfire)
stan_annotations <- read.csv(paste0(output_folder, "stan_annotations.csv")) %>%
  select(compound_name, feature) %>%
  separate_rows(feature, sep = "; ")
classy_confs <- read.csv(file = paste0(output_folder, "classy_confs.csv"))

stan_classes %>%
  left_join(stan_annotations, by="compound_name") %>%
  filter(!is.na(feature)) %>%
  separate_rows(classyfire, sep = "; ") %>%
  mutate(classyfire=str_replace(classyfire, "^.*: ", "")) %>%
  mutate(classyfire=str_replace_all(classyfire, "_", " ")) %>%
  distinct() %>%
  left_join(classy_confs, by=c(classyfire="classes", "feature")) %>%
  print(n=100)
```

```{r assessstructures}
structure_ids <- read.csv(paste0(output_folder, "structure_ids.csv"))
stan_annotations <- read.csv(paste0(output_folder, "stan_annotations.csv")) %>%
  select(compound_name, feature) %>%
  separate_rows(feature, sep = "; ") %>%
  left_join(structure_ids, by="feature") %>%
  # filter(compound_name=="Glycerophosphocholine") %>%
  print(n=200)

```

## Collect into the clean database

After running all the code above, we've got a bunch of separate data frames that
are a pain to work with when all we really need is two - one to hold the values
corresponding to each *peak* and one to hold the values corresponding to each
*feature*.

Peak values: mz, rt, area, norm_area, filename

Feature values: mzmed, rtmed, avgarea, formula, classes, structure/id

These data frames, combined with metadata_complete, can then be left joined
together and filtered/selected based on what information is actually needed.
This makes the stats scripts a lot cleaner and avoids needing to recalculate
medmz, medrt, etc. every single time.

We begin with peakvals, which starts out as the xcms output raw_peaks.
Step 1: remove adducts/isotopes named in addiso_features
Step 2: calculate norm_area using the chosen_BMIS and IS_peaks

```{r peakvals, purl=TRUE}
# Step 0: load relevant data
M_area_peaks <- read.csv(paste0(output_folder, "M_area_peaks.csv")) %>%
  select(feature, mz, rt, M_area, filename)
chosen_BMIS <- read.csv(paste0(output_folder, "chosen_BMIS.csv"))
is_peaks <- read.csv(paste0(output_folder, "is_peaks.csv")) %>%
  select(compound_name, filename, is_area=M_area)
is_addiso <- read.csv(paste0(output_folder, "is_addiso.csv"))

# Step 1
M_area_peaks <- M_area_peaks %>%
  filter(!feature%in%is_addiso$feature)

# Step 2
norm_peaks <- M_area_peaks %>%
  left_join(chosen_BMIS, by="feature") %>%
  left_join(is_peaks, by = c(BMIS="compound_name", "filename")) %>%
  group_by(feature) %>%
  mutate(norm_area=(M_area/is_area)*mean(is_area, na.rm=TRUE)) %>%
  select(feature, mz, rt, norm_area, filename)

write.csv(norm_peaks, paste0(output_folder, "norm_peaks.csv"), row.names = FALSE)
```

Featurevals then builds on this new norm_peaks data frame.
Step 1: Group by feature and produce summary statistics
 - mzmed, rtmed, avgarea
Step 2: Annotate known standards with stan_annotations
Step 3: Annotate formulae with feature_formulas
Step 4: Annotate classes with classy_confs (currently removed because annoying)
Step 5: Annotate hypothetical structures with structure_ids

```{r featurevals, purl=TRUE}
# Step 0: Load everything
stan_annotations <- read.csv(paste0(output_folder, "stan_annotations.csv")) %>%
  select(compound_name, feature)
feature_formulas <- read.csv(paste0(output_folder, "feature_formulas.csv"))

# Step 1: group and summarize
norm_feats <- norm_peaks %>%
  filter(str_detect(filename, "_Smp_")) %>%
  group_by(feature) %>%
  summarize(mzmed=mean(mz, na.rm=TRUE), rtmed=mean(rt, na.rm=TRUE), avgarea=mean(norm_area, na.rm=TRUE))

# Step 2: authentic standards
norm_feats <- norm_feats %>%
  left_join(stan_annotations, by="feature")

# Step 3: formulae
norm_feats <- norm_feats %>%
  left_join(feature_formulas, by="feature")

# Step 4: classes
# 0.99 cutoff is arbitrary
if(file.exists(paste0(output_folder, "classy_confs.csv"))){
  classy_confs <- read.csv(paste0(output_folder, "classy_confs.csv"))
  clean_classes <- classy_confs %>%
    filter(conf>0.99) %>%
    group_by(feature) %>%
    summarize(classes=paste(classes, collapse = "; "))
# # Currently commented out because it's messy and super long and printing sucks
# norm_feats <- norm_feats %>%
#   left_join(clean_classes, by="feature")
  # Instead they're all being set to NA
  norm_feats$classes <- NA
} else {
  norm_feats$classes <- NA
}

# Step 5: structures
# -20 cutoff is due to Raafay's manual checks, -25 ~= 80% correct
if(file.exists(paste0(output_folder, "structure_ids.csv"))){
  classy_confs <- read.csv(paste0(output_folder, "structure_ids.csv"))
  clean_CSI <- structure_ids %>%
    filter(confidence>-25) %>%
    select(feature, CSI_name=name)
  norm_feats <- left_join(norm_feats, clean_CSI, by="feature")
} else {
  norm_feats$CSI_name <- NA
}

write.csv(norm_feats, paste0(output_folder, "norm_feats.csv"), row.names = FALSE)
```



## Cruise correspondence

Finally, it's useful to combine all of the output into a single data frame
rather than working with several little ones. The real challenge here is
asserting that a feature is the same across the different runs given differences
in retention time, m/z, and polarity. But mostly retention time.

This can only be done on output folders that have been produced by the script
above, so if you haven't run those yet then please make sure you do.

```{r correspond, purl=TRUE}
all_cruises <- c("FK", "MT", "MC")
polarities <- rep(c("pos", "neg"), each=length(all_cruises))
output_all <- paste(paste(all_cruises, polarities, sep="_"), "output/", sep = "_")

peaks_all_files <- paste0(output_all, "norm_peaks.csv")
feats_all_files <- paste0(output_all, "norm_feats.csv")

if(!all(file.exists(feats_all_files))){
  message(paste("Not all feature lists found:", paste(feats_all_files[!file.exists(feats_all_files)], collapse = ", "), "missing"))
  next # Allows running file within for loop after purling
  # Code below only runs if all files exist
}
if(!all(file.exists(peaks_all_files))){
  message(paste("Not all feature lists found:", paste(feats_all_files[!file.exists(feats_all_files)], collapse = ", "), "missing"))
  stop("Peak lists should be made simultaneously with feature lists")
}

feats_all <- lapply(feats_all_files, function(x){
  given_cruise <- str_extract(x, "FK|MT|MC")
  v <- read.csv(x)
  v$cruise <- given_cruise
  v$feature <- paste(given_cruise, toupper(str_extract(x, "pos|neg")), v$feature, sep = "_")
  v$polarity <- str_extract(x, "pos|neg")
  v
}) %>% bind_rows() %>%
  arrange(desc(avgarea))

temp_feats <- feats_all %>%
  mutate(rtmed=ifelse(cruise=="FK", rtmed+60, rtmed))
output_groups <- list()
pb <- txtProgressBar(min = 0, max = nrow(temp_feats), style = 3)
while(nrow(temp_feats>1)){
  row_data <- temp_feats[1,]
  # if(!is.na(row_data$compound_name)){
  #   if(str_detect(row_data$compound_name, "13C12")){
  #     dput(row_data)
  #     stop()
  #   }
  # }
  out_group <- temp_feats %>%
    filter(polarity==row_data$polarity) %>%
    filter(cruise!=row_data$cruise) %>%
    filter(mzmed%between%pmppm(row_data$mzmed, 5)) %>%
    mutate(dist_from_main=map2_dbl(rtmed, avgarea, function(rtmed, avgarea){
      rt_dist <- abs(log2(row_data$rtmed)-log2(rtmed))
      area_dist <- abs(log10(row_data$avgarea)-log10(avgarea))
      sum(rt_dist, area_dist)
    })) %>%
    arrange(dist_from_main)
  if(nrow(out_group>0)){
    out_group <- out_group %>%
      filter(rtmed%between%(row_data$rtmed+c(-60, 60))) %>%
      group_by(cruise) %>%
      slice(1) %>%
      select(-dist_from_main) %>%
      ungroup() %>%
      add_row(row_data) %>%
      mutate(rtmed=ifelse(cruise=="FK", rtmed-60, rtmed))
  } else {
    out_group <- row_data
  }
  temp_feats <- temp_feats[!temp_feats$feature%in%out_group$feature,]
  output_groups[[length(output_groups)+1]] <- out_group
  setTxtProgressBar(pb, nrow(feats_all)-nrow(temp_feats))
}
close(pb)

feat_groups_init <- output_groups %>%
  map2_dfr(seq_along(.), function(group, id){
    cbind(group, group_feat=sprintf("FT%04d", id))
  })
```

```{r fix correspondence, purl=TRUE}
(same_feat_diff_names <- feat_groups_init %>%
  group_by(group_feat) %>%
  summarise(n=length(unique(compound_name[!is.na(compound_name)])),
            nms=paste0(unique(compound_name), collapse = ";")) %>%
  filter(n>1))

(same_name_diff_feats <- feat_groups_init %>%
  group_by(compound_name, polarity) %>%
  summarise(n=length(unique(group_feat)),
            nms=paste0(unique(group_feat), collapse = ";")) %>%
  filter(n>1 & !is.na(compound_name)))

# Fixes will go here if necessary
feat_groups <- feat_groups_init %>%
  select(feature, group_feat)
```

```{r final peak list compilation, purl=TRUE}
all_cruises <- c("FK", "MT", "MC")
polarities <- rep(c("pos", "neg"), each=length(all_cruises))

all_peak_list <- list()
all_feat_list <- list()
for(cruise in all_cruises){
  for(polarity in polarities){
    peaklist_path <- paste(cruise, polarity, "output/norm_peaks.csv", sep = "_")
    all_peak_list[[cruise]][[polarity]] <- peaklist_path %>%
      read_csv(show_col_types = FALSE) %>%
      mutate(polarity=polarity) %>%
      mutate(cruise=cruise) %>%
      mutate(feature=paste(cruise, toupper(polarity), feature, sep = "_"))
    
    featlist_path <- paste(cruise, polarity, "output/norm_feats.csv", sep = "_")
    all_feat_list[[cruise]][[polarity]] <- featlist_path %>%
      read_csv(show_col_types = FALSE) %>%
      mutate(polarity=polarity) %>%
      mutate(cruise=cruise) %>%
      mutate(feature=paste(cruise, toupper(polarity), feature, sep = "_"))
  }
  
  neg_dupes <- all_feat_list[[cruise]]$pos %>%
    pmap_dfr(function(...){
      row_data <- data.frame(...)
      all_feat_list[[cruise]]$neg %>%
        filter(mzmed%between%pmppm(row_data$mzmed-1.007276*2, 10)) %>%
        filter(rtmed%between%(row_data$rtmed+c(-5, 5))) %>%
        distinct(feature)
    })
  all_feat_list[[cruise]]$neg <- all_feat_list[[cruise]]$neg %>%
    anti_join(neg_dupes, by = "feature")
  all_peak_list[[cruise]]$neg <- all_peak_list[[cruise]]$neg %>%
    anti_join(neg_dupes, by = "feature")
}

all_peaks <- bind_rows(lapply(all_peak_list, bind_rows)) %>%
  select(feature, filename, norm_area)
all_feats <- bind_rows(lapply(all_feat_list, bind_rows)) %>%
  left_join(feat_groups)

write.csv(all_peaks, file = "all_peaks.csv", row.names = FALSE)
write.csv(all_feats, file = "all_feats.csv", row.names = FALSE)
```

